The next step in the simulation is to handle collisions between particles.
Solving collision constraints between particles by comparing every pair of
particles to one another is a very time consuming process,
$\mathcal{O}(n^{2})$. As such it is often desired to look at smarter ways of
solving collisions between particles. Due to the fact that particles and other
objects for that matter only interact in small regions it is well suited to
look at methods involving spatial subdivisions.

\subsubsection{Uniform Grid}

One such method that also performs well and is easy to parallelize is the use
of a \textit{Uniform Grid} \cite{Green}. A uniform grid implies that the world
is composed of a number of cells in a cubical grid, where each cell can store
particles. If the cell width equals the diameter of a particle (under the
assumption that all particles have the same diameter) collisions only occur
between particles of neighbouring cells. This means that only 27 cells per
particle needs to be considered in a three dimensional space if the particle
collisions are to be solved in a particle based manner. This type of grid is
also convenient when solving fluid constraints that originates from the ideas
behind SPH, as those require all neighbouring particles within a certain
distance (defined by the kernel width of \textit{Spiky} and \textit{Poly6}
kernels).

\subsection{Solving Collision Constraints}

For solving collision constraints we use the method in
\cite{Green} that are based upon sorting grid cells of a uniform grid. First is
the buffer that is going to store the cell ids filled with the maximum value of
an unsigned integer.  Then, per particle, is a cell id computed and stored in
the cell ids buffer at the location represented by the index of the particle.
The choice of cell id can for example be done by a linear indexing of the cells
or as we do, by using a space filling curve. We use the \textit{Z-order curve}
that is based on calculations of \textit{Morton codes} as it will increase the
spatial locality of nieghbouring particles in the buffer \cite{Green}. Higher
spatial locality between particles is great as the use of textures then will
give more cache hits when we perform calculations per particle involving its
neighbours.

When each particle has received a cell id, the cell ids buffer is sorted
increasingly. The index of each particle is also sorted as part of the sorting
process, i.e. we sort by key, where the cell id is key and the particle index
is value. This, as it will enable reordering of all other buffers and textures
as well when the sorting is done, allowing for more cache hits to happen and
for more convenient reads and writes inside the code.

Now all particles in each grid cell lie next to each other in memory. To then
find neighbouring particles it is convenient to know the start and end of each
cell. These are found by launching kernels per particle, where each particle
compares it's cell id with the cell id of the previous and next particle to see
if they are equal. If the previous particle's cell id is different from this
particle's cell id we know that the starting index of the cell for this
particle is the index of this particle. Vice versa then applies to finding the
ending index of a cell.

Given the cell endings and cell starts the k-nearest neighbours of a particle
can be found. To find the neighbours we first need to find the neighbouring
cells. This is trivial with a linear indexing. But with a space filling curve
we need to first compute a new position and then apply the space filling
curve's hash function to find out the index of that cell. I.e. by adding cell
widths to the position of a particle (in x, y, and z) we can compute the cell
id for that position, the cell id can then be used to find the beginning of
that cell, allowing us to loop through all the particles inside that cell. This
as we know when to stop looping due to us previously deriving the ending index
of the cell.

Once we found the neighbouring particles of each particle there are several
methods that can be applied to solve the collision constraints between them. We
have implemented two different methods. The first method implemented uses
atomics and can either be executed per constraint or per particle. We prefer
the per constraint manner where the following happens.

\begin{enumerate}
\item Each particle $p$ stores it's neighbours in a buffer.
\item Launch kernels for each neighbouring particle $q$ in the buffer.
\item From the index of the element particle $p$ can be derived.
\item We now know that particles $p$ and $q$ are affected by this constraint.
\item Solve the collision constraint and update both $\mathbf{x}_{p}$ and $\mathbf{x}_{p}^{*}$.
\end{enumerate}

Only particle $p$ should be updated as there are duplicates of every collision
constraint, it will namely exist another constraint where $p$ and $q$ are
swapped. Both the actual position ($\mathbf{x}_{p}$) and the predicted position
($\mathbf{x}_{p}^{*}$) are updated as we are only interested in separating the
particles and not to perform a physical collision. To perform read and write
(update) of the positions we must involve atomics as there can be several
threads writing to the same memory address at the same time.

The second method we implemented to solve collision constraints is based on
constructing a number of batches (maximum number of batches needed equals the
number of possible collisions per particle) \cite{bullet}, where each batch
contains a set of collision constraints that can be solved in parallel without
the use of atomics.
